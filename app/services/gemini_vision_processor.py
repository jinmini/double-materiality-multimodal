"""
Gemini Vision API ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ ì„œë¹„ìŠ¤
"""

import logging
import time
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import asyncio # Added for asyncio.wait_for

from app.infrastructure.clients.pdf_converter import PDFConverter
from app.infrastructure.clients.gemini_client import GeminiClient
from app.dependencies.clients import get_cost_manager_client

logger = logging.getLogger(__name__)

class GeminiVisionDocumentProcessor:
    """Gemini Vision API ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, gemini_client: GeminiClient, pdf_converter: PDFConverter = None):
        """
        Args:
            gemini_client: Gemini API í´ë¼ì´ì–¸íŠ¸
            pdf_converter: PDF ë³€í™˜ê¸° (ê¸°ë³¸ê°’: 200 DPI)
        """
        self.gemini_client = gemini_client
        self.pdf_converter = pdf_converter or PDFConverter(dpi=200)
        self.cost_manager = get_cost_manager_client()
        
        # ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ í‚¤ì›Œë“œ
        self.materiality_keywords = [
            "ì¤‘ëŒ€ì„±", "materiality", "ì¤‘ìš”ë„", "ì´í•´ê´€ê³„ì", "stakeholder",
            "ë§¤íŠ¸ë¦­ìŠ¤", "matrix", "ì´ìŠˆ", "issue", "ì˜í–¥ë„", "ê´€ì‹¬ë„"
        ]
    
    async def process_document(self, pdf_path: str) -> Dict[str, Any]:
        """
        PDF ë¬¸ì„œë¥¼ Gemini Vision APIë¡œ ë¶„ì„í•˜ì—¬ ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ
        
        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        logger.info(f"ğŸ” Gemini Vision ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
        start_time = time.time()
        
        try:
            # 1. PDF ì •ë³´ ì¡°íšŒ
            pdf_info = self.pdf_converter.get_pdf_info(pdf_path)
            logger.info(f"ğŸ“„ PDF ì •ë³´: {pdf_info['total_pages']}í˜ì´ì§€, {pdf_info['file_size']/1024/1024:.1f}MB")
            
            # 2. í˜ì´ì§€ ìˆ˜ ì œí•œ (ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•)
            max_pages = min(pdf_info['total_pages'], 10)  # ìµœëŒ€ 10í˜ì´ì§€ë§Œ ì²˜ë¦¬
            logger.info(f"ğŸ” ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜: {max_pages} (ìµœëŒ€ 10í˜ì´ì§€ ì œí•œ)")
            
            # 3. PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ (ì œí•œëœ í˜ì´ì§€ë§Œ)
            logger.info("ğŸ–¼ï¸ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹œì‘")
            page_images = self.pdf_converter.convert_specific_pages(
                pdf_path, 
                list(range(1, max_pages + 1))
            )
            
            # 4. ë¹ ë¥¸ ì¤‘ëŒ€ì„± í‰ê°€ í˜ì´ì§€ ì‹ë³„ (í‚¤ì›Œë“œ ê¸°ë°˜)
            logger.info("ğŸ” ì¤‘ëŒ€ì„± í‰ê°€ í˜ì´ì§€ ì‹ë³„ ì¤‘")
            materiality_pages = self._identify_materiality_pages_fast(page_images)
            
            if not materiality_pages:
                logger.warning("âš ï¸ ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                # ì¤‘ê°„ í˜ì´ì§€ë“¤ì—ì„œ ë¶„ì„ ì‹œë„ (ë³´í†µ ì¤‘ëŒ€ì„± í‰ê°€ëŠ” ì¤‘ê°„ì— ìœ„ì¹˜)
                start_idx = max(0, len(page_images) // 3)
                end_idx = min(len(page_images), (len(page_images) * 2) // 3)
                materiality_pages = page_images[start_idx:end_idx][:3]  # ìµœëŒ€ 3í˜ì´ì§€
            
            # 5. ì œí•œëœ í˜ì´ì§€ì—ì„œë§Œ ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ
            logger.info(f"ğŸ¯ {len(materiality_pages)}ê°œ í˜ì´ì§€ì—ì„œ ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ")
            materiality_issues = await self._extract_materiality_issues_with_timeout(materiality_pages)
            
            # 6. ESG ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            esg_categorized = self._categorize_esg_issues(materiality_issues)
            
            # 7. ê²°ê³¼ êµ¬ì„±
            processing_time = time.time() - start_time
            
            result = {
                "file_info": {
                    "filename": Path(pdf_path).name,
                    "file_id": Path(pdf_path).stem,
                    "processed_at": datetime.now().isoformat(),
                    "processing_time": f"{processing_time:.2f}ì´ˆ"
                },
                "document_analysis": {
                    "total_pages": pdf_info["total_pages"],
                    "analyzed_pages": len(materiality_pages),
                    "page_limit_applied": max_pages < pdf_info["total_pages"],
                    "has_images": pdf_info["has_images"],
                    "has_text": pdf_info["has_text"]
                },
                "materiality_issues": materiality_issues,
                "esg_content_summary": {
                    category: len(issues) 
                    for category, issues in esg_categorized.items()
                },
                "esg_categorized": esg_categorized,
                "extraction_method": "gemini_vision_optimized",
                "extraction_confidence": {
                    "score": min(0.9, len(materiality_issues) * 0.1),
                    "level": "high" if len(materiality_issues) > 5 else "medium",
                    "issues_found": len(materiality_issues),
                    "has_tables": True
                }
            }
            
            logger.info(f"âœ… Gemini Vision ì²˜ë¦¬ ì™„ë£Œ: {len(materiality_issues)}ê°œ ì´ìŠˆ ì¶”ì¶œ ({processing_time:.2f}ì´ˆ)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Gemini Vision ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            raise Exception(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    def _identify_materiality_pages_fast(self, page_images: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë¹ ë¥¸ ì¤‘ëŒ€ì„± í‰ê°€ í˜ì´ì§€ ì‹ë³„ (í…ŒìŠ¤íŠ¸ìš© 1-5í˜ì´ì§€ íŒŒì¼ì— ìµœì í™”)"""
        total_pages = len(page_images)
        
        if total_pages <= 5:
            # 5í˜ì´ì§€ ì´í•˜ë©´ ëª¨ë“  í˜ì´ì§€ ë¶„ì„ (í…ŒìŠ¤íŠ¸ íŒŒì¼ ëŒ€ì‘)
            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ íŒŒì¼ ê°ì§€ ({total_pages}í˜ì´ì§€), ëª¨ë“  í˜ì´ì§€ ë¶„ì„")
            return page_images
        
        # 5í˜ì´ì§€ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë§Œ ë²”ìœ„ ë¶„ì„ ì ìš©
        logger.info(f"ğŸ“„ í° ë¬¸ì„œ ({total_pages}í˜ì´ì§€), ë²”ìœ„ ë¶„ì„ ì ìš©")
        start_idx = max(0, int(total_pages * 0.2))
        end_idx = min(total_pages, int(total_pages * 0.8))
        candidate_pages = page_images[start_idx:end_idx]
        final_pages = candidate_pages[:8]
        
        logger.info(f"âœ… í˜ì´ì§€ ë²”ìœ„ ê¸°ë°˜ìœ¼ë¡œ {len(final_pages)}ê°œ ì¤‘ëŒ€ì„± í‰ê°€ í›„ë³´ í˜ì´ì§€ ì„ ì •")
        return final_pages
    
    async def _extract_materiality_issues_with_timeout(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """íƒ€ì„ì•„ì›ƒì´ ì ìš©ëœ ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ"""
        all_issues = []
        
        for i, page in enumerate(pages):
            try:
                logger.info(f"ğŸ¯ í˜ì´ì§€ {page['page_number']} ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ ({i+1}/{len(pages)})")
                
                # ê° í˜ì´ì§€ë‹¹ 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                issues = await asyncio.wait_for(
                    self._extract_issues_from_page(page),
                    timeout=30.0
                )
                all_issues.extend(issues)
                
                # ì¶©ë¶„í•œ ì´ìŠˆë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                if len(all_issues) >= 10:
                    logger.info(f"âœ… ì¶©ë¶„í•œ ì´ìŠˆ ë°œê²¬ ({len(all_issues)}ê°œ), ì²˜ë¦¬ ì¡°ê¸° ì¢…ë£Œ")
                    break
                    
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page['page_number']} ì²˜ë¦¬ íƒ€ì„ì•„ì›ƒ (30ì´ˆ ì´ˆê³¼)")
                continue
            except Exception as e:
                logger.warning(f"âš ï¸ í˜ì´ì§€ {page['page_number']} ì´ìŠˆ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ì œ
        unique_issues = self._deduplicate_issues(all_issues)
        
        # ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í–ˆì„ ë•Œ í´ë°± ì‹œë„
        if len(unique_issues) == 0 and len(pages) > 0:
            logger.warning("âš ï¸ 1ì°¨ ë¶„ì„ì—ì„œ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í•¨, ë” ê´€ëŒ€í•œ ë¶„ì„ ì‹œë„")
            unique_issues = await self._fallback_general_analysis(pages[:3])  # ì²˜ìŒ 3í˜ì´ì§€ë§Œ
        
        return unique_issues
    
    async def _fallback_general_analysis(self, pages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¼ë°˜ì ì¸ ESG ì½˜í…ì¸  ë¶„ì„ (í´ë°±)"""
        fallback_prompt = """
        ì´ í˜ì´ì§€ì—ì„œ ESGì™€ ê´€ë ¨ëœ ëª¨ë“  ë‚´ìš©ì„ ì°¾ì•„ì£¼ì„¸ìš”.
        ì¤‘ëŒ€ì„± í‰ê°€ê°€ ëª…ì‹œì ìœ¼ë¡œ ì—†ë”ë¼ë„, ESG ê´€ë ¨ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

        ì°¾ì„ ë‚´ìš©:
        - í™˜ê²½ ê´€ë ¨: íƒ„ì†Œë°°ì¶œ, ì—ë„ˆì§€, íê¸°ë¬¼, ìˆ˜ìì› ë“±
        - ì‚¬íšŒ ê´€ë ¨: ì•ˆì „, ì¸ê¶Œ, ë‹¤ì–‘ì„±, ì§€ì—­ì‚¬íšŒ ë“±  
        - ì§€ë°°êµ¬ì¡° ê´€ë ¨: ì´ì‚¬íšŒ, ìœ¤ë¦¬, ë¦¬ìŠ¤í¬ê´€ë¦¬ ë“±

        JSON í˜•íƒœë¡œ ì‘ë‹µ:
        [
            {
                "issue_name": "ë°œê²¬ëœ ESG ì´ìŠˆëª…",
                "esg_category": "E" | "S" | "G",
                "stakeholder_interest": "ì•Œ ìˆ˜ ì—†ìŒ",
                "business_impact": "ì•Œ ìˆ˜ ì—†ìŒ",
                "priority": "ë³´í†µ",
                "description": "ê°„ë‹¨í•œ ì„¤ëª…",
                "confidence": "ë‚®ìŒ"
            }
        ]
        """
        
        all_fallback_issues = []
        
        for page in pages:
            try:
                response = await self.gemini_client.analyze_image_with_text(
                    image_base64=page["image_base64"],
                    prompt=fallback_prompt,
                    model_name="gemini-2.0-flash",
                    max_tokens=2000
                )
                
                content = response.get("content", "").strip()
                
                # JSON ì¶”ì¶œ
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "[" in content and "]" in content:
                    start = content.find("[")
                    end = content.rfind("]") + 1
                    content = content[start:end]
                
                content = content.replace("'", '"')
                issues = json.loads(content)
                
                for issue in issues:
                    issue["source_page"] = page["page_number"]
                    issue["confidence_score"] = 0.3  # ë‚®ì€ ì‹ ë¢°ë„
                    issue["extraction_type"] = "fallback"
                
                all_fallback_issues.extend(issues)
                logger.info(f"âœ… í´ë°± ë¶„ì„: í˜ì´ì§€ {page['page_number']}ì—ì„œ {len(issues)}ê°œ ESG ì½˜í…ì¸  ë°œê²¬")
                
            except Exception as e:
                logger.warning(f"âš ï¸ í´ë°± ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
                continue
        
        return self._deduplicate_issues(all_fallback_issues)
    
    def _contains_materiality_keywords(self, page: Dict[str, Any]) -> bool:
        """í˜ì´ì§€ì— ì¤‘ëŒ€ì„± í‰ê°€ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì„ì‹œ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” OCRì´ í•„ìš”í•˜ì§€ë§Œ, ì¼ë‹¨ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ì¶”ì •
        # ë³´í†µ ì¤‘ëŒ€ì„± í‰ê°€ëŠ” ë³´ê³ ì„œ ì¤‘ê°„ ë¶€ë¶„ì— ìœ„ì¹˜
        page_num = page["page_number"]
        total_pages = 50  # ì„ì‹œê°’
        
        # ì¤‘ê°„ 50% í˜ì´ì§€ ë²”ìœ„ì—ì„œ ì°¾ê¸°
        middle_start = int(total_pages * 0.3)
        middle_end = int(total_pages * 0.8)
        
        return middle_start <= page_num <= middle_end
    
    async def _is_materiality_page(self, page: Dict[str, Any]) -> bool:
        """Gemini Visionìœ¼ë¡œ ì¤‘ëŒ€ì„± í‰ê°€ í˜ì´ì§€ì¸ì§€ í™•ì¸"""
        prompt = """
        ì´ í˜ì´ì§€ê°€ ì§€ì†ê°€ëŠ¥ê²½ì˜ë³´ê³ ì„œì˜ ì¤‘ëŒ€ì„± í‰ê°€(Materiality Assessment) ì„¹ì…˜ì¸ì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.
        
        ë‹¤ìŒ ìš”ì†Œë“¤ì„ í™•ì¸í•´ì£¼ì„¸ìš”:
        1. "ì¤‘ëŒ€ì„±", "materiality", "ì¤‘ìš”ë„" ë“±ì˜ ìš©ì–´
        2. ë§¤íŠ¸ë¦­ìŠ¤ ë˜ëŠ” ê·¸ë˜í”„ í˜•íƒœì˜ ë„í‘œ
        3. ì´í•´ê´€ê³„ì ê´€ì‹¬ë„/ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ ì¶•
        4. ESG ì´ìŠˆë“¤ì˜ ë‚˜ì—´
        
        ë‹¨ìˆœíˆ "ì˜ˆ" ë˜ëŠ” "ì•„ë‹ˆì˜¤"ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = await self.gemini_client.analyze_image_with_text(
                image_base64=page["image_base64"],
                prompt=prompt,
                model_name="gemini-2.0-flash",
                max_tokens=10
            )
            
            return "ì˜ˆ" in response.get("content", "").strip().lower()
            
        except Exception as e:
            logger.warning(f"âš ï¸ í˜ì´ì§€ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            return False
    
    async def _extract_issues_from_page(self, page: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ì¤‘ëŒ€ì„± ì´ìŠˆ ì¶”ì¶œ"""
        prompt = """
        ì´ í˜ì´ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ESG ì¤‘ëŒ€ì„± ì´ìŠˆë“¤ì„ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.

        **ì°¾ì„ ë‚´ìš©:**
        - ì¤‘ëŒ€ì„± í‰ê°€, ì¤‘ëŒ€ì„± ë§¤íŠ¸ë¦­ìŠ¤ì— í¬í•¨ëœ ì´ìŠˆë“¤
        - í‘œ, ë¦¬ìŠ¤íŠ¸, ê·¸ë˜í”„ì— ë‚˜íƒ€ë‚œ ESG ì´ìŠˆë“¤
        - í™˜ê²½(E), ì‚¬íšŒ(S), ì§€ë°°êµ¬ì¡°(G) ê´€ë ¨ ëª¨ë“  ë‚´ìš©

        **JSON í˜•íƒœë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:**
        ```json
        [
            {
                "issue_name": "ì´ìŠˆëª…",
                "esg_category": "E",
                "priority": "ë†’ìŒ",
                "description": "ì„¤ëª…"
            },
            {
                "issue_name": "ë‹¤ë¥¸ ì´ìŠˆëª…", 
                "esg_category": "S",
                "priority": "ë³´í†µ",
                "description": "ì„¤ëª…"
            }
        ]
        ```

        **ì£¼ì˜ì‚¬í•­:**
        - í˜ì´ì§€ì— ë³´ì´ëŠ” ëª¨ë“  ESG ê´€ë ¨ ë‚´ìš©ì„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ì¶”ì¶œí•˜ì„¸ìš”
        - ì‘ì€ í…ìŠ¤íŠ¸ë‚˜ í‘œ ì•ˆì˜ ë‚´ìš©ë„ ì£¼ì˜ê¹Šê²Œ í™•ì¸í•˜ì„¸ìš”
        - esg_categoryëŠ” "E", "S", "G" ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
        - priorityëŠ” "ë†’ìŒ", "ë³´í†µ", "ë‚®ìŒ" ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
        - ë°˜ë“œì‹œ ìœ íš¨í•œ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”

        ì´ì œ ì´ í˜ì´ì§€ë¥¼ ë¶„ì„í•´ì„œ ESG ì´ìŠˆë“¤ì„ JSONìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = await self.gemini_client.analyze_image_with_text(
                image_base64=page["image_base64"],
                prompt=prompt,
                model_name="gemini-2.0-flash",  # ìµœì‹  ëª¨ë¸ ì‚¬ìš©
                max_tokens=3000  # ë” ë§ì€ í† í° í—ˆìš©
            )
            
            # JSON íŒŒì‹± ì‹œë„
            content = response.get("content", "").strip()
            logger.info(f"ğŸ” Gemini ì‘ë‹µ (ì²˜ìŒ 200ì): {content[:200]}...")
            
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                # ì¼ë°˜ì ì¸ ì½”ë“œ ë¸”ë¡
                parts = content.split("```")
                for part in parts:
                    if "[" in part and "]" in part:
                        content = part.strip()
                        break
            elif "[" in content and "]" in content:
                start = content.find("[")
                end = content.rfind("]") + 1
                content = content[start:end]
            
            # JSON íŒŒì‹± ì „ ì •ë¦¬
            content = content.replace("'", '"')  # ì‘ì€ë”°ì˜´í‘œë¥¼ í°ë”°ì˜´í‘œë¡œ ë³€í™˜
            content = content.replace("ï¼Œ", ",")  # ì¤‘êµ­ì–´ ì‰¼í‘œ ë³€í™˜
            
            logger.info(f"ğŸ” ì •ì œëœ JSON: {content[:300]}...")
            
            issues = json.loads(content)
            logger.info(f"âœ… í˜ì´ì§€ {page['page_number']}ì—ì„œ {len(issues)}ê°œ ì´ìŠˆ ì¶”ì¶œ")
            
            # í˜ì´ì§€ ì •ë³´ ë° ê¸°ë³¸ê°’ ì¶”ê°€
            for issue in issues:
                issue["source_page"] = page["page_number"]
                # ê¸°ë³¸ê°’ ì„¤ì •
                if "stakeholder_interest" not in issue:
                    issue["stakeholder_interest"] = "ë³´í†µ"
                if "business_impact" not in issue:
                    issue["business_impact"] = "ë³´í†µ"
                if "confidence" not in issue:
                    issue["confidence"] = "ë†’ìŒ"
                
                # ì‹ ë¢°ë„ ì ìˆ˜í™”
                confidence_text = issue.get("confidence", "ë†’ìŒ")
                if confidence_text == "ë†’ìŒ":
                    issue["confidence_score"] = 0.9
                elif confidence_text == "ë³´í†µ":
                    issue["confidence_score"] = 0.7
                else:
                    issue["confidence_score"] = 0.5
            
            return issues
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            logger.warning(f"ì‘ë‹µ ë‚´ìš©: {content[:500]}...")
            return []
        except Exception as e:
            logger.error(f"âŒ ì´ìŠˆ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _deduplicate_issues(self, issues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì´ìŠˆ ì œê±°"""
        seen_names = set()
        unique_issues = []
        
        for issue in issues:
            issue_name = issue.get("issue_name", "").strip().lower()
            if issue_name and issue_name not in seen_names:
                seen_names.add(issue_name)
                unique_issues.append(issue)
        
        logger.info(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(issues)}ê°œ â†’ {len(unique_issues)}ê°œ")
        return unique_issues
    
    def _categorize_esg_issues(self, issues: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """ESG ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì´ìŠˆ ë¶„ë¥˜"""
        categorized = {
            "í™˜ê²½(E)": [],
            "ì‚¬íšŒ(S)": [],
            "ì§€ë°°êµ¬ì¡°(G)": [],
            "ê¸°íƒ€": []
        }
        
        for issue in issues:
            esg_category = issue.get("esg_category", "").upper()
            if esg_category == "E":
                categorized["í™˜ê²½(E)"].append(issue)
            elif esg_category == "S":
                categorized["ì‚¬íšŒ(S)"].append(issue)
            elif esg_category == "G":
                categorized["ì§€ë°°êµ¬ì¡°(G)"].append(issue)
            else:
                categorized["ê¸°íƒ€"].append(issue)
        
        return categorized 